---
title: "Paper review: VisualPhishNet"
tags: application
---

작성자: 심소연





# VisualPhishNet 논문 리뷰



안녕하세요? 저는 이번에 블로그 글 작성을 맡게 된 20181144 심소연이라고 합니다. 

저는 논문 리뷰를 통해 머신 러닝이 보안 분야에서 어떻게 사용될 수 있는지에 대해 간략히 알아보는 글을 작성하고자 합니다. 제가 리뷰할 논문은 'VisualPhishNet: Zero-Day Phishing Website Detection by Visual Similarity' 입니다. 논문은 [여기](https://dl.acm.org/doi/pdf/10.1145/3372297.3417233?casa_token=rHdsszNWt9QAAAAA:-SCiE1BP8cXP1JNUj8KliPdIdjpRDKnrvPOa0Ti4ZENHK4eZ3TLMlMoVef1HMJfBGI_Y6xyCbuDeXJY)에서 보실 수 있습니다. 

---



이 논문은 시각적 유사성을 기반으로 피싱 사이트를 탐지할 수 있는 기술인 **VisualPhishNet**을 다루었습니다.

> ✅ 피싱 사이트란? 
>
> 실제와 유사한 웹 페이지를 통해 사용자의 개인정보 및 금융정보를 요구한 뒤 각종 공격, 특히 금전적 피해를 일으키는 사기 수법을 의미합니다.



유사성을 기반으로 피싱 사이트를 감지하는 선행 연구에 대해 언급하기 전, 대략적인 시각적인 유사성을 이용한 피싱 사이트 탐지 과정에 대해 짚고 넘어가겠습니다.

1. 먼저 (피싱 사이트의 표적이 되는) 신뢰할 수 있는 웹 페이지들의 리스트를 생성합니다.
2. 목록에는 없는 웹 페이지를 방문할 때 마다 해당 웹 페이지와 리스트 상의 웹 페이지들과 비교합니다.
3. 만약 시각적 유사성이 높다고 판별되면 리스트에 속하는 웹 페이지를 가장한 피싱 사이트로 분류하게 됩니다.

이 방법의 장점은 우회하기 쉬운 휴리스틱에 의존하는 것이 아니라, 공격자의 전략에 (즉, 신뢰할 수 있는 웹 페이지를 최대한으로 모방하여 사용자를 눈속임하겠다는 전략) 의존하기 때문에 우회에 덜 취약하다는 것입니다. 하지만 단점도 있습니다. 신뢰할 수 있는 웹 페이지 리스트의 크기가 작기 때문에 소수의 웹 사이트에 대한 공격만을 탐지할 수 있습니다. 또한 공격자의 전략과는 다른, 부분적으로만 유사한 피싱 사이트를 제작하여 유사성 기반의 피싱 사이트 탐지를 우회할 수 있다는 위험이 도사리고 있습니다.



## 선행 연구

유사성 기반의 피싱 사이트 탐지에 대한 선행 연구에 대해 알아보도록 하겠습니다.

1. HTML 기반의 피싱 사이트 탐지

   (Huang et al., , Zhang et al., Liu et al.,  Rosiello et al., Mao et al.) 피싱 페이지와 표적이 된 신뢰할 수 있는 웹 페이지 사이에는 HTML 측면의 유사성이 있을 수 있어 이를 피싱 사이트 탐지에 이용합니다.

    ⚠ 하지만, 이 탐지 기법은 공격자가 HTML 텍스트 대신 이미지 혹은 embedded object를 사용할 경우 우회할 수 있습니다. 또한, 코드 난독화 기법으로도 우회할 수 있습니다.

2. 스크린샷 기반의 피싱 사이트 탐지

   (Chen et al.) 스크린 샷의 레이아웃 기반

   스크린샷의 분할된 블록을 비교하여 레이아웃의 유사성을 측정하는 방법이 있습니다. 

   ⚠ 하지만 이 방법에는 복잡한 페이지를 분할하는 것은 어렵다는 단점이 있습니다.

3. 스크린 샷에서의 특징적인 시각적 요소를 기반으로 한 피싱 사이트 탐지

   1. (Afroz et al.) SIFT (scale-invariant feature transform)를 이용한 로고 일치 여부 확인

      SIFT 알고리즘은 이미지에서 특징을 추출하는 대표적인 알고리즘 중 하나입니다. 이 알고리즘은 이미지의 크기와 회전에 불변하는 특징을 추출하기 때문에 이미지의 크기가 변하거나 회전을 하더라도 일관된 특징을 추출할 수 있습니다. 

      이미지에 다양한 크기의 변형을 적용한 후 특징을 추출하기 위해 가우시안 흐림을 적용하게 되는데, 흐림 효과가 적용된 이미지 간의 차이로부터 특징적인 포인트를 추출하고, 이미지의 전체적인 방향까지 추출하게 돼 최종적으로는 크기 및 회전에 불변하는 특징을 얻게 됩니다. ([출처](https://ballentain.tistory.com/47)) 

      보통 가우시안 흐림 효과는 이미지에서의 불필요한 노이즈를 제거하는 데 도움이 되므로, 이미지의 가장자리를 추출하는 데 사용되곤 합니다.

   2. (Rao et al.) SURF (speed-up robust features), (Bozkir et al.) HOG (histogram of oriented gradients) 를 이용한 스크린샷의 특징 일치 여부 확인

      SURF 알고리즘은 SIFT 알고리즘의 객체인식 속도가 느리다는 단점을 보완한 알고리즘입니다. 그리고 HOG 알고리즘은 픽셀의 변화량의 각도와 크기를 고려하여 히스토그램 형태의 특징을 추출하는 알고리즘입니다.

4. URL 기반의 피싱 사이트 탐지

   1. (Woodbridge et al.) Siamese CNN을 이용한 시각적으로 유사한 URL 탐지

      렌더링된 URL에 대한 트레이닝을 통해, 시각적으로 유사한 URL을 탐지하는 기술입니다.

      샴 네트워크 (Siamese Network)는 두 사진을 입력으로 받고, 동일한 두 개의 CNN 네트워크로 두 이미지로부터 비주얼 임베딩을 각각 추출합니다. 추출된 비주얼 임베딩 벡터간의 거리를 구해서 이를 0과 1사이의 유사도 값으로 출력합니다. 이 네트워크에서 차별화된 점은 뉴럴넷의 복수 사용과 특별한 손실 함수인 triplet loss를 사용한다는 점입니다. ([출처](https://wikidocs.net/150813)) 

      triplet loss는 세 개의 이미지로부터 계산되는 손실 함수입니다. 이 손실 함수의 값을 최소화하도록 학습을 진행하게 되면, 결국 같은 클래스에 속하는 이미지의 거리는 작아지게 되고 다른 클래스에 속하는 이미지의 거리는 커지게 됩니다. 손실 함수에 $\alpha$라는 인자를 추가하게 되는데, 이 이유는 ‘같은 클래스’와 ‘다른 클래스’ 간의 차이를 분명히 하기 위함입니다. ([출처](https://3months.tistory.com/507))

   ⚠ 하지만 이 탐지 기술에는 모양이 같거나 비슷해 쉽게 구분할 수 없는 글자를 의미하는 호모글리프 공격을 해결할 수 없다는 문제가 있습니다. 

위에서 언급한 문제와 더불어 동일한 웹 사이트가 다양한 시각적인 특징을 보이는 하위 웹 페이지들을 보유하고 있다면, 악용의 여지가 발생하게 됩니다. 이것은 선행 연구들은 페이지 대 페이지 일치 여부에 집중하였기 때문입니다. 

그러나 VisualPhishNet은 서로 다른 페이지 대 페이지 일치에 의존하지 않습니다. 대신, 두 동일한 웹 사이트 내의 서로 다른 웹 페이지 간의 유사성을 조사합니다. 따라서 선행 연구에 비해 더욱 적은 제약 조건으로 (선행 연구에는 레이아웃 일치 여부 등의 '제약 조건'이 있었습니다) 피싱 사이트를 탐지할 수 있습니다. 그리고 VisualPhishNet은 CNN을 기반으로 하였습니다.

> ✅ 이 논문에서는 웹 사이트와 웹 페이지에 대해 구별할 필요가 있습니다. 



## VisualPhishNet 구현 과정

### (1) VisualPhishNet 데이터셋 수집

학습에 사용될 데이터는 다음의 과정을 거쳐 수집됐습니다.

> ⚠ 이미 피싱 공격의 표적이 된 웹 사이트 = trusted-list 에 속하는 웹 사이트 = 신뢰할 수 있는 웹 사이트 
>
> ​	이 단어들이 혼용될 수 있으니, 유의해주세요. 

1. 먼저, 피싱 사례를 수집하기 위하여 PhishTank 에서 확인된 피싱 페이지의 스크린샷을 크롤링했습니다.

   그 결과, 저자는 다음의 통찰을 얻을 수 있었습니다.

   * 동일한 피싱 스크린샷 디자인이 여러 URL 에서 발견될 수 있음을 알 수 있었습니다. 저자는 겹치지 않는 트레이닝 및 테스트 셋을 갖기 위해, 수동으로 스크린샷을 검사해 중복을 제거하였습니다.  

   * 하나의 웹 사이트를 표적으로 하는 여러 피싱 페이지들은 웹 페이지 요소의 위치, 색상, 크기, 언어 등에서 차이가 남을 확인할 수 있었습니다. 이러한 피싱 셋은 이러한 변화에 대한 모델의 견고함을 시험하는 데 사용될 수 있을 것입니다.

   * 일부 피싱 페이지들은 전체적인 디자인에서 봤을 때 표적 웹 사이트와의 유사성이 떨어진다는 점에서 엉성하게 디자됨을 알 수 있었습니다. 사실 이전의 연구에서는 이러한 예시는 제외되었으나, VisualPhishNet은 이를 포함하였습니다.

2. 이미 표적이 된 신뢰할 수 있는 웹 페이지의 (즉, trusted-list에 속하는 웹 페이지의) 모든 내부 링크를 크롤링했습니다. 

   이 과정 덕분에 선행 연구에서는 탐지할 수 없었던 피싱 페이지까지 탐지할 수 있게 됩니다. 

3. 추가적으로 유명한 웹 사이트에 대한 스크린샷을 크롤링하고 나서, 일부 중복된 trusted-list 웹 페이지는 제외하였습니다.

   유명한 웹 사이트의 기준은 Alexa top 500 및 SimilarWeb top 100 에 속하는 웹 사이트가 되겠습니다. 또한 은행, 금융, 정부 서비스 등 피싱에 가장 취약한 카테고리에 속하는 100개의 웹 사이트도 포함하였습니다.

4. 위의 과정으로 스크린샷을 모은 후, 데이터셋을 분리하였습니다.

   [리마인더] 피싱 페이지를 다른 무해한 웹 사이트와 구별하는 것이 트레이닝의 최종적인 목적입니다.

   - 신뢰할 수 있는 페이지 목록의 (trusted-list) 웹 페이지에 대한 스크린샷 (A)
   - 신뢰할 수 있는 페이지 목록을 (trusted-list) 대상으로 제작된 피싱 사이트에 대한 스크린샷 (B)
   - 신뢰할 수 있는 페이지 목록에 (trusted-list) 없는 또다른 합법적인 사이트에 대한 스크린샷 (C)

5. trusted-list에 대한 분석을 진행하였습니다.

   이 과정은 공격자가 탐지를 우회하기 위해 아직 표적이 되지 않은 다른 웹 사이트를 표적으로 삼아 공격하는 행위를 사전에 막기 위함입니다. 저자는 PhishTank trusted-list 그리고 피싱 공격에 취약하다고 예상되는 사이트 간의 교차를 계산하였는데, 피싱 공격에 취약하다고 예상되는 사이트는 PhishTank trusted-list의 88%를 커버하였습니다.

### (2) VisualPhishNet 구현

기존의 분류기는 임의의 페이지가 trusted-list에 속하는지에 대한 분류를 진행했습니다. 하지만 이 경우, 아직 표적이 되지 않은 새로운 웹 사이트를 분류할 때 오류가 발생하기 쉽다는 단점이 있습니다.

따라서, 이 연구에서는 <mark>같은 웹 사이트 내의 하위 웹 페이지 간의 유사성</mark>을 학습하는 것을 목표로 했습니다.

#### triplet networks

Triplet network는 Siamese networks의 확장판입니다. triplet network는 anchor image, positive image, negative image를 필요로 합니다. 

- anchor image
- positive image: anchor image와 본질이 동일한 이미지입니다.
- negative image: anchor image와 본질이 다른 이미지입니다.

VisualPhishNet에서는 anchor image 및 positive image는 동일한 웹 사이트 내의 서로 다른 웹 페이지 스크린샷에, 그리고 negative image는 anchor image와는 다른 웹 사이트의 스크린샷에 대응됩니다. 이 네트워크의 최종 목표는 특징 공간에서 anchor image, positive image간의 거리가 anchor image, negative image 간의 거리보다 작은 특징 공간을 학습하는 것입니다. 

#### loss function

Triplet network에서 사용하는 손실 함수는 다음과 같고, 손실 함수를 최소화하는 것을 학습의 목표로 설정합니다.

$Loss= \sum\limits_i ^N \max ( ||f(x_i^a) -f(x^p_i)||^2_2-||f(x_i^a) -f(x^n_i)||^2_2+\alpha,0)$

여기서 $\alpha$는 거리의 차가 $\alpha$보다 작은 경우에 대해 패널티를 주기 위한 값입니다.

#### shared network 생성

VisualPhishNet은 VGG16을 선행학습에 사용했습니다. VGG16의 차별화된 점은 3*3 필터를 한 번이 아닌 세 번이나 반복 적용한다는 것입니다. ([출처](https://medium.com/@msmapark2/vgg16-논문-리뷰-very-deep-convolutional-networks-for-large-scale-image-recognition-6f748235242a)) VGG16을 사용하기 위해 모든 스크린샷은 가로 224px, 세로 224px로 재설정됐습니다. 그리고 컨볼루션 레이어 다음에 완전히 연결된 레이어를 사용하는 대신, 로고 등의 로컬 패턴을 감지하는 작업에 더육 적합한 GMP (global max pooling) 레이어를 사용했습니다. VGG 이미지 크기와 일치시키기 위해 모든 스크린샷은 RGB 채널을 사용하여 224x224로 크기가 조정되었습니다.

### (3) triplet sampling

학습 시 무작위적인 샘플링 대신 'hard sampling'을 사용했습니다. 왜냐하면 Loss function 의 값을 0으로 만들어버리는 많은 데이터셋이 학습에는 도움이 되지 않기 때문입니다. hard sampling 과정은 다음과 같습니다.

1. 랜덤으로 샘플을 추출하여 쿼리 셋에 추가합니다.

2. 랜덤으로 추출한 샘플에 대한 hard positive example, 그리고 hard negative example을 query set 에 추가합니다.

   여기서 hard positive example이란 동일한 웹 사이트에 속하는 샘플 중 가장 큰 거리를 갖는 샘플을 의미하며, hard negative example이란 각기 다른 웹 사이트에 속하는 샘플 중 가장 작은 거리를 갖는 샘플을 의미합니다.

3. 과적합을 막기 위해 위의 과정을 반복합니다.

### (4) 예측

trusted-list 웹 페이지와의 L2 distance가 정의된 임계값보다 작으면 해당 페이지는 시각적 유사도가 높아 trusted-list 웹 페이지를 가장한 피싱 사이트로 분류됩니다. 반면, trusted-list 웹 페이지와의 L2 distance가 작지 않다면 trusted-list에 속하지 않는 또다른 합법적인 웹 사이트로  분류됩니다.

과연 거리의 임계값은 어떻게 설정되었을까요?

1. 피싱 페이지와 trusted-list 사이트 간의 최소 거리를 계산합니다.

2. 합법적인 페이지와 trusted-list 사이트간의 최소 거리를 계산합니다.

3. 위 결과를 히스토그램으로 나타내서 임계값을 결정합니다.

   거리의 임계값이 8일 때 분류의 성능은 true positive rate = 93%, false positive rate = 4% 이었습니다.

 

## VisualPhishNet 평가

VisualPhishNet은 adam optimizer을 사용하여 학습이 이루어졌습니다. 그리고 학습에 사용된 $\alpha$의 값은 2.2였습니다.

### (1) 제거 연구

학습에 사용된 요인을 하나씩 제거한 결과로 나타나는 영향을 평가했습니다. 그 결과, triplet network의 성능이 Siamese network의 성능을 능가함을 알 수 있었습니다. 그리고 hard example 을 통해 성능을 향상시킬 수 있었음을 확인할 수 있었습니다. 또한, 트레이닝 셋을 20% 줄였을 때 성능의 저하는 미세했기 때문에 분류기가 견고함을 보일 수 있었습니다.

### (2) trusted-list 확장

피싱 사이트에 대한 데이터셋은 PhishTank list에 포함된 웹 사이트에 대응되는 피싱 사이트만을 대상으로 수집했기 때문에, PhishTank를 제외한 사이트에서 제공하는 웹 페이지 목록 데이터셋은 학습에서 방해요소로 작용하게 됩니다. 이러한 새로운 웹 사이트를 학습에 추가하였을 때 성능이 감소했습니다만 그 정도가 미세했기 때문에 이를 통해 학습 시킨 모델이 견고함을 보일 수 있었습니다.

### (3) 이전의 작업과의 비교

이전의 이미지 매칭 방법은 (SURF, HOG, ORB) trusted-list에서는 확인할 수 없는 새로운 시각적인 특성을 가진 피싱 페이지에 대해서는 효과적이지 않았습니다. 그리고 VisualPhishNet이 VGG16과 ResNet10의 예측 성능을 능가하였습니다.

### (4) embeddings 시각화

> ✅ embedding
>
> 특징을 추출해서 컴퓨터가 학습할 수 있도록 수치화하는 작업을 의미합니다. ([출처](https://velog.io/@dongho5041/딥러닝-인공신경망의-Embedding이란))

learned feature space 생성 과정은 다음과 같습니다.

1. VisualPhishNet 은 각각의 스크린샷에 대해 특징 벡터(차원: 512)를 생성합니다.

2. t-SNE (t-distributed stochastic neighbor embedding)으로 특징 벡터의 차원을 2차원으로 줄입니다.

3. 특징 벡터의 시각화 결과, 다음의 사실을 확인할 수 있었습니다.

   학습된 특징 공간 상에서 '동일한 웹 사이트 내의 페이지들의 거리' < '서로 다른 웹 사이트 간의 페이지의 거리' 임을 확인할 수 있었습니다. 또한, 피싱 페이지는 또다른 합법적인 웹 페이지보다 trusted-list 웹 페이지와 더욱 가까움을 확인할 수 있었습니다.

### (5) 견고성 및 보안 평가

논문의 저자는 VisualPhishNet이 공격에서도 여전히 견고한지를 평가하기 위해 두 가지 우회 공격을 제시했습니다. 두 공격 모델에서의 공격자의 공통적인 목적은 분류 모델의 무결성을 위반하는 것이라고 가정하며, 사용자에게 신뢰를 얻기 위해서는 엉성한 디자인을 하지 않았음을 가정했습니다.

* 입력의 작은 변화 (예: 색상, 노이즈 및 위치의 변경)

ROC curve 에서 약간의 성능 저하가 발생함을 확인할 수 있었으나, 이것은 학습 중 데이터 증대를 통해 보완할 수 있습니다.

* 화이트박스 공격 (공격자가 학습에 사용된 데이터에 전적으로 접근할 수 있음을 가정함을 의미합니다.)

  적대적 공격이란, 딥러닝 모델의 내부 취약점을 이용하여, 인간의 눈으로는 거의 감지할 수 없는 노이즈를 이용하여 의도적으로 분류기를 속이는 공격입니다. ([출처1](https://rain-bow.tistory.com/entry/Adversarial-Attack), [출처2](https://www.bosch-ai.com/research/research-applications/universal-adversarial-perturbations/))  특히 VisualPhishNet의 견고함을 평가하기 위해 적대적 공격 중 하나인 FGSM (fast gradient sign method)이 사용됐습니다.

  FGSM에서는 적대적인 인풋을 생성하기 위해 다음의 식을 이용했습니다.

  $\tilde{x} = x + \epsilon \text{sign} (\nabla _x J(\theta, x, y))$

  - $x$: 학습에 사용된 인풋

  - $\tilde {x}$: 적대적인 인풋

  - $\epsilon \text{sign} (\nabla _x J(\theta, x, y))$: 학습에 사용된 인풋에 첨가될 노이즈입니다.

    - $\epsilon$: 노이즈 (노이즈가 클수록 변형을 인식하기 쉬워집니다.)

    - $\text{sign}$: 부호 함수 (원래는 cost의 기울기와 반대 방향으로 업데이트하여 cost를 최소화하지만, 공격의 경우 cost를 크게 만들어 모델이 인풋을 제대로 분류하지 못하도록 만드는 것이 최종 목표입니다.)

    - $y$: 정답 라벨

    - $\theta$: 모델의 파라미터

    - $J$: 트레이닝에 사용된 비용 함수

      비용 함수(손실 함수)를 계산할 때 피싱 페이지를 anchor image, 공격 대상이 된 trusted-list 사이트를 positive image, 그리고 또 다른 신뢰할 수 있는 웹 사이트의 페이지를 negative image로 설정하였습니다.

두 번째 공격을 수행할 때, hard example을 거친 경우에 한해서 모델의 견고함을 유지할 수 있었다고 합니다. 따라서 논문의 저자는 시각적 유사성 기반의 피싱 사이트 탐지 과정 중 hard example의 중요성을 보일 수 있었습니다.

### (6) 새로운 크롤링 데이터를 이용한 평가

Zero-day page를 이용하였으며, 이 페이지들은 실험을 다 진행하고 나서 추가된 것들입니다. VGG-16 matching 정확도는 65.8%인 반면 VisualPhishNet 정확도는 93.25% 으로 여전히 높은 정확도를 보여줍니다.



## VisualPhishNet에 관한 논의

### 성공적인 사례

VisualPhishNet은 다음의 경우에서도 성공적으로 피싱 유무를 예측할 수 있었습니다.

1. trusted-list 를 거의 똑같이 복제한 경우
2. trusted-list 에서 약간의 변형을 준 경우
3. trusted-list 와 레이아웃이 유사하지만 내용이 크게 변형된 경우
4. trusted-list 와 디자인이 크게 다른 경우

### 실패한 사례

하지만 VisualPhishNet은 다음의 경우에서는 성공적으로 피싱 유무를 예측할 수 없었습니다.

1. 표적 웹 사이트와 비교하여 디자인의 품질이 크게 떨어지는 피싱 사이트

2. 이전 버전의 디자인을 갖는 피싱 사이트

3. trusted-list 에 포함된 사이트의 로고를 포함하는 기사

   대표적인 예로는, Facebook 로고를 포함한 Facebook 관련 기사를 보도하는 것이 되겠습니다.

---



이 논문의 차별화된 점을 요약해서 말하자면, 원본 사이트와 피싱 사이트 간의 단순 유사도 차이만을 학습하는 것이 아니라, 원본 사이트의 하위 웹 페이지 간의 유사성까지 학습하여 피싱 사이트를 구별할 수 있다는 것입니다. 같은 웹사이트 내의 웹 페이지 또한 학습의 대상이 되므로, 피싱 사이트의 우회 기술에도 흔들리지 않는 견고한 분류기 성능을 보여줄 수 있습니다. 

저는 이 논문을 읽으면서 방해가 되는 요소를 학습에 추가한다는 시도가 참신하다고 생각했습니다. 제가 준비한 글은 여기까지이고, 읽어주셔서 감사합니다 🙂